# 25-10-08 Corvin

## 1. Zielsetzung

Ziel der Arbeit ist es, die **Word Error Rate (WER)** verschiedener **Whisper-Modelle (tiny, base, small)** auf Basis von **akustischen Features** vorherzusagen.

---

## 2. Datengrundlage

Die Trainingsdaten stammen aus den Common Voice Delta Segmente 20 und 21. Für diese Daten wurden 342 akustische Features berechnet und die die WER-Werte der drei Whisper-Modelle:
- `wer_tiny`
- `wer_base`
- `wer_small`

Je Whisper-Modell wurden optimierte Feature-Datasets erstellt:

| Whisper-Version | Zielspalte | Output-Ordner | Methoden |
|------------------|------------|----------------|-----------|
| Tiny  | `wer_tiny`  | `results/datasets/selected_tiny_full`  | Correlation, Mutual Information (MI), PCA |
| Base  | `wer_base`  | `results/datasets/selected_base_full`  | Correlation, Mutual Information (MI), PCA |
| Small | `wer_small` | `results/datasets/selected_small_full` | Correlation, Mutual Information (MI), PCA |

(!!!) Leckage wurde eliminiert: alle `wer_*`-Spalten außer der Zielspalte werden konsequent ausgeschlossen. (!!!)

---

## 3. Modelle & Transformationen

Die Modellierung umfasst:
- **Modelle:** Ridge, Lasso, RandomForest, GradientBoosting, HistGradientBoosting, SVR  
- **Zieltransformationen:**  
  - none  
  - yeo-johnson  
  - log1p  
  - boxcox  

---

## 4. Ergebnisse – Top 5 pro Whisper-Modell

### Tiny
| Rang | Dataset | Modell | Transform | R² |
|------|----------|---------|-----------|----|
| 1 | dataset_mi_top14.csv | Random Forest | none | 0.0821 |
| 2 | dataset_mi_top15.csv | Random Forest | none | 0.0753 |
| 3 | dataset_mi_top17.csv | Random Forest | none | 0.0732 |
| 4 | dataset_mi_top15.csv | Random Forest | log1p | 0.0732 |
| 5 | dataset_mi_top18.csv | Random Forest | log1p | 0.0726 |

**Interpretation:**  
Random Forest ist am stabilsten, Mutual Information liefert die besten Features, Transformationen bringen keinen Zugewinn.

---

### Base
| Rang | Dataset | Modell | Transform | R² |
|------|----------|---------|-----------|----|
| 1 | dataset_pca_top80.csv | Random Forest | log1p | 0.0715 |
| 2 | dataset_pca_top50.csv | Random Forest | log1p | 0.0697 |
| 3 | dataset_pca_top90.csv | Random Forest | log1p | 0.0696 |
| 4 | dataset_mi_top100.csv | Random Forest | log1p | 0.0689 |
| 5 | dataset_corr_top90.csv | Random Forest | log1p | 0.0684 |

**Interpretation:**  
Random Forest dominiert, PCA leicht stabiler als MI, log1p-Transformation bringt etwas Zugewinn.

---

### Small
| Rang | Dataset | Modell | Transform | R² |
|------|----------|---------|-----------|----|
| 1 | dataset_mi_top70.csv | Gradient Boosting | log1p | 0.0772 |
| 2 | dataset_mi_top20.csv | HistGradientBoosting | none | 0.0726 |
| 3 | dataset_mi_top40.csv | Gradient Boosting | none | 0.0711 |
| 4 | dataset_mi_top60.csv | Random Forest | log1p | 0.0643 |
| 5 | dataset_mi_top100.csv | Random Forest | log1p | 0.0642 |

**Interpretation:**  
Gradient Boosting-Modelle erzielen die besten Ergebnisse, MI-Feature-Auswahl liefert beste Features.

---

## 5. Nächste Schritte

### 5.1 Tabular Baseline (LightGBM & CatBoost)
Ziel: robuste, leistungsfähige Modelle für tabulare Daten.

**Geplantes Vorgehen:**
- Modelle: LightGBM, CatBoost  
- Features: gleiche Selektionen wie oben (MI, PCA, Correlation)
- Ziel: WER vorhersagen (tiny, base, small)

**Erweiterungen:**
- Feature-Importance-Analyse mit SHAP  
- Ablation: *cheap* (nur Basis-Features) vs. *+teure* (inkl. DNSMOS/NISQA etc.)

---

### 5.2 Embedding-basierter Ansatz (Transfer Learning)

Zweiter Pfad zur Nutzung tiefer akustischer Repräsentationen.

**Pipeline:**
- Pretrained Encoder: WavLM, HuBERT, wav2vec2 (Base)
- Encoder eingefroren, kein Finetuning notwendig
- Frame-Pooling (Mean/Std) → 768D Embedding
- MLP-Head (2–3 Dense Layers)
- Loss: MSE oder Beta-Regression (mit Wortgewichtung)
- Optional: Nutzung von DNSMOS/NISQA als Zusatzfeatures

---

### 5.3 Weitere Optionen
- Ensemble aus besten Tabular- und Embedding-Modellen
- Vergleich interpretativer Kennzahlen (z. B. SHAP, Permutation Importance)
- Stabilitätsanalyse über verschiedene Subsets oder SNR-Level

---
